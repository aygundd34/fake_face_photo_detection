# -*- coding: utf-8 -*-
"""fake face detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmWodEYAPL52Q7Nu8k5QksJYQr8JWfQH
"""

import os
import sys

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import cv2

import albumentations as A
from albumentations.pytorch import ToTensorV2

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import ToTensor
from torchvision import models
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

real_dir = '/content/drive/MyDrive/data_source/data/real-and-fake-face-detec/real-vs-fake/real-vs-fake/train/real/'
real_path = os.listdir(real_dir)

fake_dir = '/content/drive/MyDrive/data_source/data/real-and-fake-face-detec/real-vs-fake/real-vs-fake/train/fake/'
fake_path = os.listdir(fake_dir)

"""**Görüntüleri Yükleme ve Görselleştirme** işlemi yapılıyor. Görüntüleri bir dizinden yüklemek ve renk uzaylarını BGR'den RGB'ye dönüştürmek için bir load_img işlevi tanımlanıyor. Daha sonra matplotlib.pyplot kullanarak gerçek ve sahte yüzleri yüklemek ve görselleştirmek için bu işlev kullanılıyor."""

#Daha iyi görselleştirme için dizinden görüntüler yükleniyor ve renk alanı CV2 standart BGR den RGB ye dönüştürülüyor.
def load_img(path):

    image = cv2.imread(path) #cv2.imread(), belirtilen dosyadan bir görüntü yükler.
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #BGR(Mavi, Yeşil, Kırmızı) RGB'ye(Kırmızı, Yeşil, Mavi) dönüştürülür.
    return image_rgb

fig = plt.figure(figsize=(10, 10)) #Realdeki fotoğrafların size ları 10*10 olarak değiştirildi.

for i in range(25):
    plt.subplot(5, 5, i+1) # Fotoğraflar için 5*5 lik tablo oluşturma
    plt.imshow(load_img(real_dir + real_path[i])) #Fotoğrafın ekranda görüntülenmesi
    plt.suptitle("Real faces", fontsize=20)#Başlık ve fontunun ayarlanması
    plt.axis('off')# Fotoğraf kenarındaki ölçekli çerçevenin kaldırılması

fig = plt.figure(figsize=(10, 10)) #Fakedeki fotoğrafların size ları 10*10 olarak değiştirildi.

for i in range(25):
    plt.subplot(5, 5, i+1) # Fotoğraflar için 5*5 lik tablo oluşturma
    plt.imshow(load_img(fake_dir + fake_path[i])) #Fotoğrafın ekranda görüntülenmesi
    plt.suptitle("Fake faces", fontsize=20) #Başlık ve fontunun ayarlanması
    plt.axis('off') # Fotoğraf kenarındaki ölçekli çerçevenin kaldırılması

"""Burada oluşturulan iki veri çerçevesi var bunlar real_df ve fake_df.    real_df işlevi kullanılarak oluşturulur. pd.DataFrame ve gerçek görüntüler hakkında bilgi tutar. İki sütunu var: "image_path" ve "label". real_dir"image_path" sütunu , (gerçek görüntülerin depolandığı dizin) ve real_path(gerçek görüntülerin dosya adlarının listesi) birleştirilerek elde edilen her bir gerçek görüntünün yolunu içerir. "Etiket" sütunu, bu görüntülerin gerçek olduğunu belirten 1 olarak ayarlanmıştır.
Benzer şekilde, fake_df sahte görüntülerin yollarını içeren ve "etiket" sütununu 0 olarak ayarlayan sahte görüntüler için oluşturulur.

"""

real_df = pd.DataFrame({'image_path': real_dir + real_path[i], 'label': 1} for i in range(0, 1000))#Real ler için dataframe oluşturma
fake_df = pd.DataFrame({'image_path': fake_dir + fake_path[i], 'label': 0} for i in range(0, 1000))#Fake ler için dataframe oluşturma

"""**pd.concat:** Her iki veri çerçevesi hem gerçek hem de sahte görüntüler hakkında bilgi içeren tek bir veri çerçevesi oluşturmak için işlev kullanılarak birleştirilir df. Parametre ignore_index=True, birleştirilmiş veri çerçevesinin dizininin sıfırlanmasını sağlar."""

df = pd.concat([real_df, fake_df],ignore_index=True) #Concat ile real_df ve fake_df dataframe leri birleştirildi.
df.tail(10) # 10 tane satır döndürülmesi

"""**veri ön işleme:  df dataframe, sklearn işlevi kullanılarak karıştırılır shuffleve yöntem kullanılarak dizin sıfırlanır reset_index, bu da veri çerçevesindeki görüntü yollarının ve etiketlerin rastgele sıralanmasıyla sonuçlanır."""

df = shuffle(df) # Dataframe deki satırların karıştırılması
df = df.reset_index(drop=True) #Dizini sıfırladığımızda eski dizin sütun olarak eklenir ve yeni bir sıralı dizin kullanılır,
                               #Eski dizinin sütun olarak eklenmesini önlemek için drop parametresini kullanılır.
df.head(10) # İlk 10 satırı görüntüle

"""**train_test_split:** sklearn.model_selection modülünden bir fonksiyondur. Veri kümesini eğitim ve doğrulama kümelerine bölmek için kullanılır.
**df:** Bölmek istediğimiz girdi veri çerçevesi.
**test_size=0.2:** Doğrulama kümesi için ayrılması gereken veri kümesi oranını belirtir. Bu durumda, verilerin %20'si doğrulama için kullanılacaktır.
**random_state=42:** Tekrarlanabilirlik için rastgele tohumu ayarlar. Belirli bir değer sağlayarak (bu durumda 42), kod her çalıştırıldığında aynı rastgele bölünme elde edilecektir, bu da sonuçların tekrarlanabilirliği için yararlıdır.
**train_df, val_df:** Bunlar bölme işleminden sonra ortaya çıkan iki veri çerçevesidir. train_df eğitim verilerini, val_df ise doğrulama verilerini içerir.
Bu kod satırını çalıştırdıktan sonra, train_df eğitim için rastgele seçilen orijinal verilerin %80'ini ve val_df doğrulama için verilerin kalan %20'sini içerir. Bölme işlemi tabakalıdır, yani hem eğitim hem de doğrulama kümelerinde aynı oranda gerçek ve sahte görüntü var.
"""

train_df, val_df = train_test_split(df, test_size=0.3, random_state=42) # Veri setinin test ve train olarak bölünmesi.

"""image_size = 224:  Veri kümesindeki görüntüler için istenen boyutu (piksel) gösterilir. Görüntüler, eğitim veya çıkarım için modele beslenmeden önce bu boyuta yeniden boyutlandırılır.

batch_size = 64: Eğitim sırasında her yinelemede işlenecek görüntü sayısını belirtir. Yani model bir seferde 64 görüntü alacak ve işleyecek.

num_epochs = 10: Eğitim sırasında tüm veri kümesinin kaç kez yineleneceğinin sayısıdır. Veri kümesinden geçen her tam geçişe bir çağ denir. Yani, bu durumda, model 10 dönem için eğitilecektir.

device = 'cuda' if torch.cuda.is_available() else 'cpu': Modelin üzerinde eğitileceği ve çalıştırılacağı cihazı (CPU veya GPU) belirler. Bir GPU varsa, diziyi 'cuda'değişkene atar device; aksi halde  'cpu' atar. Bu, CPU'lara kıyasla eğitim sürecini önemli ölçüde hızlandırabildikleri için, GPU'ların hesaplama gücünde daha kullanışlıdır.

device: Eğitim ve çıkarım için seçilen cihazı gösteren değişkenin değerini gösterir . 'cuda' ya da 'cpu'  olabilir .
"""

#Modelin üzerinde eğitileceği ve çalıştırılacağı cihazı (CPU veya GPU) belirlenir.
image_size = 224
batch_size = 64
num_epochs = 3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""'train_transform': Bu, eğitim sırasında kullanılan dönüştürme işlem hattının anahtarıdır.

A.Compose([...]): Birden fazla görüntü dönüşümünün bir bileşimini oluşturulması. Dönüşümler sıralı olarak uygulanır.

A.Resize(image_size, image_size): Görüntüyü kare şekline yeniden boyutlandırma. Bu, eğitim setindeki tüm görüntülerin aynı boyuta sahip olmasını sağlar.

A.HorizontalFlip(p=0.5): Görüntü üzerinde 0,5 olasılıkla yatay bir çevirme yapılması. Bu, farklı perspektifler sağlayarak eğitim verilerinin çeşitliliğini artırmaya yardımcı olur.

A.RandomBrightnessContrast(always_apply=False, p=0.4): Görüntünün parlaklığına ve kontrastına rastgele değişiklikler uygular. p=0,4 parametresi, bu dönüşümün uygulanma olasılığının %40 olduğunu gösterir.

A.Solarize(always_apply=False, p=0.4, threshold=(42, 42)): Belirli bir eşiğin üzerindeki piksel değerlerini ters çeviren solarizasyonu görüntüye uygular. Eşik (42, 42) olarak ayarlanmıştır, bu da 42'nin üzerindeki piksel değerlerinin ters çevrileceğini gösterir. Bu dönüşüm 0,4 olasılıkla uygulanır.

A.MultiplicativeNoise(always_apply=False, p=0.8, multiplier=(0.6800000071525574, 1.409999966621399), per_channel=True, elementwise=True): Görüntüye çarpımsal gürültü ekler, bu da piksel değerlerinde rastgele değişimler meydana getirir. Gürültü, görüntünün her kanalına (RGB) bağımsız olarak uygulanır. p=0,8 parametresi bu dönüşümün uygulanma şansının %80 olduğunu belirtir.

A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0): Ortalama değerleri (0,485, 0,456, 0,406) çıkararak ve standart sapma değerlerine (0,229, 0,224, 0,225) bölerek görüntüyü normalleştirir. Bu normalleştirme adımı, modelin eğitim sırasında daha hızlı yakınsamasına yardımcı olur. max_pixel_value=255.0 parametresi piksel değerlerinin 0 ile 1 arasında ölçeklenmesini sağlar. Bu dönüşüm 1.0 (her zaman) olasılıkla uygulanır.

ToTensorV2(): Görüntüyü bir PyTorch tensörüne dönüştürür. PyTorch modelleri tensör formatında girdi verileri beklediğinden bu gereklidir.

Diğer iki dönüşüm, 'validation_transform' ve 'visualization_transform', benzer bir yapı izler. Ancak, doğrulama veya görselleştirme sırasında genellikle istenmediğinden, çevirme veya çarpımsal gürültü gibi belirli artırmaları içermezler. Tutarlılığı sağlamak için öncelikle yeniden boyutlandırma ve normalleştirme dönüşümlerini içerirler.

Bu dönüşüm ardışık düzenlerini tanımlayarak, gereksinimlerinize bağlı olarak eğitim, doğrulama veya görselleştirme aşamalarında belirtilen dönüşümleri görüntülerinize kolayca uygulayabilirsiniz.
"""

# Fotoğrafların eğitim, doğrulama ve görselleştirme dönüşümlerinin yapılması
image_transforms = {'train_transform': A.Compose([A.Resize(image_size, image_size),
                                                  A.HorizontalFlip(p=0.5),
                                                  A.RandomBrightnessContrast(always_apply=False,
                                                                             p=0.4),
                                                  A.Solarize(always_apply=False,
                                                             p=0.4,
                                                             threshold=(42, 42)),
                                                  A.MultiplicativeNoise(always_apply=False,
                                                                        p=0.8,
                                                                        multiplier=(0.6800000071525574, 1.409999966621399),
                                                                        per_channel=True,
                                                                        elementwise=True),
                                                  A.Normalize(mean=(0.485, 0.456, 0.406),
                                                              std=(0.229, 0.224, 0.225),
                                                              max_pixel_value=255.0,
                                                              p=1.0),
                                                  ToTensorV2()]),

                   'validation_transform': A.Compose([A.Resize(image_size, image_size),
                                                      A.Normalize(mean=(0.485, 0.456, 0.406),
                                                                  std=(0.229, 0.224, 0.225),
                                                                  max_pixel_value=255.0,
                                                                  p=1.0),
                                                      ToTensorV2()]),
                   'visualization_transform': A.Compose([A.Resize(image_size, image_size),
                                                         A.HorizontalFlip(p=0.5),
                                                         A.RandomBrightnessContrast(always_apply=False,
                                                                                    p=0.4),
                                                  A.Solarize(always_apply=False,
                                                             p=0.4,
                                                             threshold=(42, 42)),
                                                  A.MultiplicativeNoise(always_apply=False,
                                                                        p=0.8,
                                                                        multiplier=(0.6800000071525574, 1.409999966621399),
                                                                        per_channel=True,
                                                                        elementwise=True)])}

"""**def__init** sınıfın yapıcısıdır. Sınıfın örnek değişkenlerini başlatır bunlar: image_labels, image_dir, transform ve target_transform.

**image_labels**, veri kümesindeki görüntülere karşılık gelen etiketleri içeren liste veya dizi benzeri bir nesnedir.
**image_dir**, görüntülerin bulunduğu dizini temsil eden bir dizedir.
**transform**, görüntülere uygulanabilecek bir dönüşümü temsil eden isteğe bağlı bir bağımsız değişkendir. Bu dönüşüm tipik olarak veri artırma veya ön işleme için kullanılır.
**target_transform**, etiketlere uygulanabilecek bir dönüşümü temsil eden isteğe bağlı bir bağımsız değişkendir.
**def__len**, veri kümesindeki görüntülerin sayısı olan veri kümesinin uzunluğunu döndürür. **ImageDataset** sınıfının bir örneği üzerinde len() fonksiyonunu kullandığınızda çağrılır.

ImageDataset sınıfının bir örneğinde indeksleme operatörünü kullandığınızda **def__getitem** çağrılır. Girdi olarak bir dizin alır ve bu dizine karşılık gelen görüntü ve etiketi döndürür.
"""

# sınıf oluşturuldu, self ile sınıfın özniteliklerine erişildi, davranışlar tanımlandı.
class ImageDataset(Dataset):
    def __init__(self, image_labels, image_dir, transform=None, target_transform=None):
        self.image_labels = image_labels
        self.image_dir = image_dir
        self.transform = transform
        self.target_transform = target_transform


    def __len__(self):
        return len(self.image_labels)


    def __getitem__(self, index):
        image_path = self.image_dir.iloc[index]
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        label = self.image_labels.iloc[index]
        if self.transform:
            image = self.transform(image=image)['image']
        if self.target_transform:
            label = self.target_transform(label=label)
        return image, label

"""Görüntü sınıflandırma modelini eğitmek için verilerin hazırlanması. Etiketler ve görüntü yolları veri çerçevelerinden çıkarılır ve ImageDataset sınıfı eğitim, doğrulama ve görselleştirme için veri kümeleri oluşturmak için kullanılır. image_transforms sözlüğünde belirtilen anahtarlara göre görüntülere farklı dönüşümler uygulanır.

PyTorch çerçevesini kullanarak görüntü sınıflandırması için veri kümeleri oluşturulması.

**train_label = train_df['label'] ve val_label = val_df['label']:** Sırasıyla eğitim ve doğrulama veri çerçevelerinden etiketleri çıkarır. Her iki veri çerçevesindeki etiket sütununun "label" olarak adlandırıldığını varsayar ve çıkarılan etiketleri train_label ve val_label değişkenlerine atar.

**train_features = train_df['image_path'] ve val_features = val_df['image_path']:** Benzer şekilde, bu satırlar görüntü yollarını içeren sütunun "image_path" olarak adlandırıldığını varsayarak eğitim ve doğrulama veri çerçevelerinden görüntü yollarını çıkarır. Çıkarılan yollar train_features ve val_features değişkenlerine atanır.

**train_dataset = ImageDataset(train_label, train_features, transform=image_transforms['train_transform']):** Özel bir ImageDataset sınıfı kullanarak train_dataset adında bir eğitim veri kümesi nesnesi oluşturur. ImageDataset sınıfı train_label ve train_features öğelerini girdi olarak alır. Ayrıca görüntülere uygulanacak dönüşümü temsil eden transform adlı ek bir argüman da alır. Bu durumda, dönüşüm image_transforms sözlüğünden 'train_transform' anahtarı kullanılarak elde edilir.

**val_dataset = ImageDataset(val_label, val_features, transform=image_transforms['validation_transform']):** Burada da aynı ImageDataset sınıfını kullanarak val_dataset adında bir doğrulama veri kümesi nesnesi oluşturur. Girdi olarak val_label ve val_features değerlerini alır ve image_transforms sözlüğünden 'validation_transform' anahtarıyla belirtilen dönüşümü uygular.

**visual_train_dataset = ImageDataset(train_label, train_features, transform=image_transforms['visualization_transform']):** Burda visual_train_dataset adında bir görselleştirme veri kümesi nesnesi oluşturuluyor. Aynı ImageDataset sınıfını, eğitim etiketlerini ve görüntü yollarını kullanır. Ancak, image_transforms sözlüğünden 'visualization_transform' anahtarıyla belirtilen farklı bir dönüşüm uygular.
"""

# Görüntü sınıflandırma modelini eğitmek için verilerin hazırlanması. Etiketler ve görüntü yolları veri çerçevelerinden çıkarılır.
train_label = train_df['label']
train_features = train_df['image_path']

val_label = val_df['label']
val_features = val_df['image_path']

train_dataset = ImageDataset(train_label,
                             train_features,
                             transform=image_transforms['train_transform'])
val_dataset = ImageDataset(val_label,
                           val_features,
                           transform=image_transforms['validation_transform'])
visual_train_dataset =  ImageDataset(train_label,
                                     train_features,
                                     transform=image_transforms['visualization_transform'])

"""PyTorch'taki görüntü veri kümeleri için veri yükleyicilerinin oluşturulması.

**train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True):** Bu satır train_dataset nesnesi için train_loader adında bir veri yükleyici oluşturur. PyTorch'taki DataLoader sınıfı, verilerin verimli bir şekilde toplu olarak yüklenmesini sağlayan bir yineleyici sağlar. İlk argüman olarak train_dataset'i alır. batch_size parametresi her bir yığındaki örnek sayısını belirtir. shuffle parametresi True olarak ayarlanır, bu da eğitim sürecine rastgelelik katmak için örneklerin her epoktan önce rastgele karıştırılacağı anlamına gelir.

**val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True):** Burda satır val_dataset nesnesi için val_loader adında bir veri yükleyici oluşturur. DataLoader sınıfını kullanır ve val_dataset nesnesini ilk bağımsız değişken olarak alır. batch_size ve shuffle parametreleri train_loader ile aynı şekilde ayarlanır.

**visual_loader = DataLoader(visual_train_dataset, batch_size=batch_size, shuffle=True):** Burda visual_train_dataset nesnesi için visual_loader adında bir veri yükleyici oluşturur. Ayrıca DataLoader sınıfını kullanır ve visual_train_dataset nesnesini ilk bağımsız değişken olarak alır. batch_size ve shuffle parametreleri train_loader ve val_loader ile aynı şekilde ayarlanır.

Veri yükleyiciler, eğitim veya değerlendirme sırasında veri kümeleri üzerinde mini gruplar halinde yineleme yapmak için kullanılır. Yığın boyutu belirtilerek, veri yükleyici her yinelemede bir örnek yığını yükler ve döndürür. True olarak ayarlanan shuffle parametresi, örneklerin her epoktan önce karıştırılmasını sağlar, bu da eğitim sırasında önyargıyı azaltmaya ve genellemeyi iyileştirmeye yardımcı olur.
"""

# PyTorch'taki görüntü veri kümeleri için veri yükleyicilerinin oluşturulması.
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)
visual_loader = DataLoader(visual_train_dataset, batch_size=batch_size, shuffle=True)

"""Verilen parametrelerle val_loader nesnesini oluşturarak, doğrulama veri kümesi üzerinde mini gruplar halinde yinelemek için değerlendirme döngüsünde kullanabilirsiniz. val_loader'ın her yinelemesi, değerlendirme amacıyla mini bir doğrulama örnekleri ve etiketleri grubu sağlayacaktır.
 0x7ff87853ec70 onaltılık değeri, nesnenin depolandığı bellek adresini temsil eder.
"""

val_loader

"""Görselleştirme veri kümesini temsil eden visual_loader'dan bir sonraki veri yığınını alınması. Ardından, özellik yığınının ve hedef yığının şeklini yazdırarak yığındaki tensörlerin boyutları hakkında bilgi verir. Bu, işlenmekte olan verilerin yapısını ve boyutunu anlamak içindir.

**Feature batch shape:** Dize biçimlendirmesini kullanarak özellik yığınının şeklini yazdırır

**Target batch shape:** Hedef (etiket) yığınının şeklini yazdırır
"""

# Görselleştirme veri kümesini temsil eden visual_loader'dan bir sonraki veri yığınını alınması.
visual_train_f, visual_train_t = next(iter(visual_loader))
print(f'Feature batch shape: {visual_train_f.size()}')
print(f'Target batch shape: {visual_train_t.size()}')

"""visual_loader veri yükleyicisi üzerinde yineleme yapar ve veri gruplarını alır. Her bir yığın için özellik tensörü ve etiket tensörü çıkarılır ve ayrı değişkenlerde (img ve label) saklanır. Ardından, tensörlerin içeriği yazdırılır ve yineleme sırasında her bir yığındaki verileri incelemenize olanak tanır.

**for item in visual_loader:** visual_loader veri yükleyicisi üzerinde bir döngü başlatır. Veri yükleyici boyunca yineleme yapar ve her yinelemede veri gruplarını alır.

**img, label = item[0], item[1]:** Döngünün içinde, geçerli veri grubu item değişkeninde saklanır. Yığındaki her öğe iki öğeden (özellik tensörü ve etiket tensörü) oluşan bir tuple olduğundan, bu satır iki öğeyi ayrı img ve label değişkenlerine ayırır. img özellik tensörünü, label ise geçerli yığın için etiket tensörünü içerecektir.

**print(img, label):** Her bir veri grubu için img ve label tensörlerini yazdırır. Yineleme sırasında yığındaki tensörlerin içeriğini görselleştirmeye yardımcı olur. Gerçek çıktı, belirli tensör temsiline ve print() işlevi tarafından uygulanan biçimlendirmeye bağlı olacaktır.
"""

# veri yükleyicisi üzerinde yineleme yapılır ve veri grupları alınır. Her bir yığın için özellik tensörü ve etiket tensörü çıkarılır ve
# ayrı değişkenlerde (img ve label) saklanır. Ardından,tensörlerin içeriği yazdırılıyor ve yineleme sırasında her bir yığındaki verileri incelemenizi sağlar.
for item in visual_loader:
    img, label = item[0], item[1]
    print(img, label)

""" Bir grup artırılmış görüntüyü ve bunlara karşılık gelen etiketleri alır ve matplotlib alt grafikleri kullanarak bunları ızgara benzeri bir düzenlemede görselleştirir. Artırılmış görüntüleri ve ilgili etiketleri görsel olarak incelemek için uygun bir yol sağlar.
 def plot_batch(features, target, batch_size=batch_size): üç parametre alan plot_batch işlevini tanımlanıyor: özellikler, hedef ve isteğe bağlı bir parametre olan batch_size (varsayılan olarak batch_size değişkeninin değeridir).

**plt.figure(figsize=(10, 40)): **plt.figure() kullanarak yeni bir şekil nesnesi oluşturur. figsize parametresi, şeklin genişliğini ve yüksekliğini inç cinsinden belirten (10, 40) olarak ayarlanır. Bu, görüntü yığınını görüntüleyecek grafiğin boyutunu ayarlar.

**for i in range(batch_size):** batch_size aralığı üzerinde bir döngü başlatır. Yığındaki görüntü sayısını temsil eden batch_size sayısını yineler.
img = özellikler[i] ve label = hedef[i]: Döngü içinde, özellikler ve hedef tensörlerine i indeksinden erişilerek yığındaki geçerli görüntü için img (özellik) ve etiket çıkarılır.
**plt.subplot(16, 4, i+1):** Şekil içinde bir alt grafik oluşturur. subplot() fonksiyonu, tek bir şekil içinde birden fazla alt grafik düzenlemek için kullanılır. Bu durumda, 16 satır ve 4 sütunlu bir alt grafik ızgarası oluşturur ve i+1 geçerli alt grafiğin indeksini belirtmek için kullanılır.
**plt.title(f'Class: {label}'):** Geçerli alt grafiğin başlığını görüntünün etiketi ile ayarlar.
**plt.imshow(img):** img görüntüsünü görüntüler.
**plt.show():** Tüm alt grafiklerle birlikte genel grafiği görüntüler. Artırılmış görüntü yığınını etiketleriyle birlikte gösterir.
"""

# Bir grup artırılmış görüntüyü ve bunlara karşılık gelen etiketleri alır ve matplotlib
# alt grafikleri kullanarak bunları ızgara benzeri bir düzenlemede görselleştirir.
def plot_batch(features, target, batch_size=batch_size):
    plt.figure(figsize=(10, 40))
    for i in range(batch_size):
        img = features[i]
        label = target[i]

        plt.subplot(16, 4, i+1)
        plt.title(f'Class: {label}')
        plt.imshow(img)
    plt.show()

"""**visual_train_f: **Bir grup artırılmış görüntü içeren özellik grubunu temsil eder.
**visual_train_t:** Özellik grubundaki görüntüler için karşılık gelen etiketleri içeren hedef (etiket) grubunu temsil eder.

**plot_batch(visual_train_f, visual_train_t):** Özellik ve hedef yığınlarını görselleştirme için plot_batch fonksiyonuna aktarmış olursunuz. Fonksiyon, artırılmış görüntüleri etiketleriyle birlikte görüntüleyen bir çizim oluşturacaktır.
Dolayısıyla, plot_batch(visual_train_f, visual_train_t), artırılmış görüntü yığınını ve bunlara karşılık gelen etiketleri gösteren bir çizim görüntüler.
"""

#  Etiketlenen görüntülerin çizdirilmesi
plot_batch(visual_train_f, visual_train_t)

"""FaceNet tanımlanıyor ve basit bir eğitim döngüsüyle eğitiliyor"""

import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision import transforms

# Define data augmentation transforms
train_transform = transforms.Compose([
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Apply data augmentation to training dataset
train_dataset = train_df(train_data, train_labels, transform=train_transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Rest of the code remains the same...

# Define ReduceLROnPlateau scheduler
scheduler_custom = ReduceLROnPlateau(optimizer_custom, mode='min', factor=0.1, patience=3, verbose=True)

def training_loop(model, training_loader, validation_loader, criterion, optimizer, scheduler, epochs=num_epochs):
    '''Training loop for train and eval modes'''
    best_val_loss = float('inf')
    for epoch in range(1, epochs+1):
        model.train()
        train_accuracy = 0
        train_loss = 0
        for image, target in training_loader:
            image = image.to(device)
            target = target.to(device)
            target = target.unsqueeze(1)
            optimizer.zero_grad()
            outputs = torch.sigmoid(model(image))
            loss = criterion(outputs.float(), target.float())

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_accuracy += ((outputs > 0.5) == target).float().mean().item()

        with torch.no_grad():
            model.eval()
            valid_loss = 0
            val_accuracy = 0
            for val_image, val_target in validation_loader:
                val_image = val_image.to(device)
                val_target = val_target.to(device)
                val_target = val_target.unsqueeze(1)
                val_outputs = torch.sigmoid(model(val_image))
                val_loss = criterion(val_outputs.float(), val_target.float())

                valid_loss += val_loss.item()
                val_accuracy += ((val_outputs > 0.5) == val_target).float().mean().item()

        avg_train_loss = train_loss / len(training_loader)
        avg_train_acc = train_accuracy / len(training_loader)
        avg_val_loss = valid_loss / len(validation_loader)
        avg_val_acc = val_accuracy / len(validation_loader)

        print(f'Epoch: {epoch} Train loss: {avg_train_loss:.4f} Train accuracy: {avg_train_acc:.4f} Val loss: {avg_val_loss:.4f} Val accuracy: {avg_val_acc:.4f}')

        scheduler.step(avg_val_loss)

        # Save the model if it achieves the best validation loss
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_model.pt')

# Rest of the code remains the same...

# class FaceNet(nn.Module):
#     def __init__(self):
#         super(FaceNet, self).__init__()
#         self.conv_1 = nn.Conv2d(in_channels=3, out_channels=18, kernel_size=3)
#         self.maxpool = nn.MaxPool2d(kernel_size=2)
#         self.batchnorm_1 = nn.BatchNorm2d(18)
#         self.conv_2 = nn.Conv2d(in_channels=18, out_channels=18, kernel_size=3)
#         self.batchnorm_2 = nn.BatchNorm2d(18)
#         self.conv_3 = nn.Conv2d(in_channels=18, out_channels=32, kernel_size=3)
#         self.fc_1 = nn.Linear(21632, 128)
#         self.fc_2 = nn.Linear(128, 64)
#         self.classifier = nn.Linear(64, 1)

#     def forward(self, x):
#         x = self.maxpool(nn.functional.relu(self.conv_1(x)))
#         x = self.maxpool(nn.functional.relu(self.conv_2(x)))
#         x = self.maxpool(nn.functional.relu(self.conv_3(x)))
#         x = torch.flatten(x, 1)
#         x = nn.functional.relu(self.fc_1(x))
#         x = nn.functional.relu(self.fc_2(x))
#         x = torch.sigmoid(self.classifier(x))
#         return x

# model_custom = FaceNet()
# model_custom.to(device)

# criterion_custom = torch.nn.BCELoss()
# optimizer_custom = torch.optim.Adam(model_custom.parameters(), lr=0.0001, weight_decay=1e-5)
# scheduler_custom = torch.optim.lr_scheduler.ExponentialLR(optimizer_custom, gamma=0.9)

# def training_loop(model, training_loader, validation_loader, criterion, optimizer, scheduler, epochs=num_epochs):
#     '''Eğitim ve değerlendirme modları için eğitim döngüsü'''
#     for epoch in range(1, epochs+1):
#         model.train()
#         train_accuracy = 0
#         train_loss = 0
#         for image, target in training_loader:
#             image = image.to(device)
#             target = target.to(device)
#             target = target.unsqueeze(1)
#             optimizer.zero_grad()
#             outputs = torch.sigmoid(model(image))
#             loss = criterion(outputs.float(), target.float())

#             loss.backward()
#             optimizer.step()

#             train_loss += loss.item()
#             train_accuracy += ((outputs > 0.5) == target).float().mean().item()

#         with torch.no_grad():
#             model.eval()
#             valid_loss = 0
#             val_accuracy = 0
#             for val_image, val_target in validation_loader:
#                 val_image = val_image.to(device)
#                 val_target = val_target.to(device)
#                 val_target = val_target.unsqueeze(1)
#                 val_outputs = torch.sigmoid(model(val_image))
#                 val_loss = criterion(val_outputs.float(), val_target.float())

#                 valid_loss += val_loss.item()
#                 val_accuracy += ((val_outputs > 0.5) == val_target).float().mean().item()

#         print(f'Epoch: {epoch} Train loss: {train_loss/len(training_loader)} Train accuracy: {train_accuracy /len(training_loader)} Val loss: {valid_loss/len(validation_loader)} Val accuracy: {val_accuracy/len(validation_loader)}')
#         scheduler.step()
# --------------------------------
# #  FaceNet epoch sonuçlarının görüntülenmesi
# training_loop(model_custom,
#               train_loader,
#               val_loader,
#               criterion_custom,
#               optimizer_custom,
#               scheduler_custom,
#               epochs=num_epochs)

# #  FaceNet epoch sonuçlarının görüntülenmesi
# training_loop(model_custom,
#               train_loader,
#               val_loader,
#               criterion_custom,
#               optimizer_custom,
#               scheduler_custom,
#               epochs=num_epochs)

from sklearn.metrics import confusion_matrix
import seaborn as sns

def draw_confusion_matrix(model, data_loader):
    y_true = []
    y_pred = []

    model.eval()
    with torch.no_grad():
        for images, labels in data_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = torch.sigmoid(model(images))

            predicted_labels = (outputs > 0.5).int().squeeze()
            y_pred.extend(predicted_labels.tolist())
            y_true.extend(labels.tolist())

    cm = confusion_matrix(y_true, y_pred)
    labels = ['Real', 'Fake']

    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

draw_confusion_matrix(model_custom, val_loader)

import matplotlib.pyplot as plt

def training_loop(model, training_loader, validation_loader, criterion, optimizer, scheduler, epochs=num_epochs):
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(1, epochs+1):
        model.train()
        train_accuracy = 0
        train_loss = 0
        for image, target in training_loader:
            image = image.to(device)
            target = target.to(device)
            target = target.unsqueeze(1)
            optimizer.zero_grad()
            outputs = torch.sigmoid(model(image))
            loss = criterion(outputs.float(), target.float())

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_accuracy += ((outputs > 0.5) == target).float().mean().item()

        with torch.no_grad():
            model.eval()
            valid_loss = 0
            val_accuracy = 0
            for val_image, val_target in validation_loader:
                val_image = val_image.to(device)
                val_target = val_target.to(device)
                val_target = val_target.unsqueeze(1)
                val_outputs = torch.sigmoid(model(val_image))
                val_loss = criterion(val_outputs.float(), val_target.float())

                valid_loss += val_loss.item()
                val_accuracy += ((val_outputs > 0.5) == val_target).float().mean().item()

        # Calculate average losses and accuracies
        train_loss /= len(training_loader)
        valid_loss /= len(validation_loader)
        train_accuracy /= len(training_loader)
        val_accuracy /= len(validation_loader)

        # Print the current epoch's statistics
        print(f'Epoch: {epoch} Train loss: {train_loss} Train accuracy: {train_accuracy} Val loss: {valid_loss} Val accuracy: {val_accuracy}')

        # Store the losses and accuracies for visualization
        train_losses.append(train_loss)
        val_losses.append(valid_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)

        scheduler.step()

    # Plot the accuracy and loss curves
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs+1), val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, epochs+1), val_accuracies, label='Val Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()

# ...

# Define the model, criterion, optimizer, and scheduler
model_custom = FaceNet()
model_custom.to(device)

criterion_custom = torch.nn.BCELoss()
optimizer_custom = torch.optim.Adam(model_custom.parameters(), lr=0.0001, weight_decay=1e-5)
scheduler_custom = torch.optim.lr_scheduler.ExponentialLR(optimizer_custom, gamma=0.9)

# Call the training_loop function
training_loop(model_custom, train_loader, val_loader, criterion_custom, optimizer_custom, scheduler_custom, epochs=num_epochs)

#  ResNet
model_np = models.resnet50(pretrained=False)
model_np.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=512, bias=True),
                     nn.ReLU(inplace=True),
                     nn.Linear(in_features=512, out_features=1, bias=True))

model_np.to(device)

criterion_np = torch.nn.BCELoss()
optimizer_np = torch.optim.Adam(model_np.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_np = torch.optim.lr_scheduler.ExponentialLR(optimizer_np, gamma=0.9)

training_loop(model_np,
              train_loader,
              val_loader,
              criterion_np,
              optimizer_np,
              scheduler_np,
              epochs=num_epochs)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# ...

def training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):
    # Training loop code here...

    # After training, calculate predictions on validation set
    model.eval()
    val_predictions = []
    val_targets = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            predictions = torch.round(torch.sigmoid(outputs))

            val_predictions.extend(predictions.cpu().numpy())
            val_targets.extend(labels.cpu().numpy())

    # Convert lists to numpy arrays
    val_predictions = np.array(val_predictions)
    val_targets = np.array(val_targets)

    # Calculate confusion matrix
    cm = confusion_matrix(val_targets, val_predictions)

    # Plot confusion matrix
    classes = ['Fake', 'Real']  # Replace with your class labels
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

# ...

# Define the model architecture
model_np = models.resnet50(pretrained=False)
model_np.fc = nn.Sequential(
    nn.Linear(in_features=2048, out_features=512, bias=True),
    nn.ReLU(inplace=True),
    nn.Linear(in_features=512, out_features=1, bias=True)
)

model_np.to(device)

criterion_np = torch.nn.BCELoss()
optimizer_np = torch.optim.Adam(model_np.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_np = torch.optim.lr_scheduler.ExponentialLR(optimizer_np, gamma=0.9)

# Call the training_loop function
training_loop(model_np, train_loader, val_loader, criterion_np, optimizer_np, scheduler_np, epochs=num_epochs)

import matplotlib.pyplot as plt

def training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(epochs):
        # Training loop code here...

        # After each epoch, calculate losses and accuracies
        train_loss = np.mean(train_losses)
        val_loss = np.mean(val_losses)
        train_accuracy = np.mean(train_accuracies)
        val_accuracy = np.mean(val_accuracies)

        # Print the current epoch's statistics
        print(f"Epoch: {epoch+1}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}")
        print("-----------------------------------------")

        # Store the losses and accuracies for visualization
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)

        # ...

    # Plot the accuracy and loss curves
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs+1), val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, epochs+1), val_accuracies, label='Val Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()

# ...

# Define the model architecture
model_np = models.resnet50(pretrained=False)
model_np.fc = nn.Sequential(
    nn.Linear(in_features=2048, out_features=512, bias=True),
    nn.ReLU(inplace=True),
    nn.Linear(in_features=512, out_features=1, bias=True)
)

model_np.to(device)

criterion_np = torch.nn.BCELoss()
optimizer_np = torch.optim.Adam(model_np.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_np = torch.optim.lr_scheduler.ExponentialLR(optimizer_np, gamma=0.9)

# Call the training_loop function
training_loop(model_np, train_loader, val_loader, criterion_np, optimizer_np, scheduler_np, epochs=num_epochs)

model_p = models.resnet50(pretrained=True)
model_p.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=512, bias=True),
                     nn.ReLU(inplace=True),
                     nn.Linear(in_features=512, out_features=1, bias=True))
for param in model_p.parameters():
    param.requires_grad = True
model_p.to(device)

criterion_p = torch.nn.BCELoss()
optimizer_p = torch.optim.Adam(model_p.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=0.9)

training_loop(model_p,
              train_loader,
              val_loader,
              criterion_p,
              optimizer_p,
              scheduler_p,
              epochs=num_epochs)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# ...

def training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):
    # Training loop code here...

    # After training, calculate predictions on validation set
    model.eval()
    val_predictions = []
    val_targets = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            predictions = torch.round(torch.sigmoid(outputs))

            val_predictions.extend(predictions.cpu().numpy())
            val_targets.extend(labels.cpu().numpy())

    # Convert lists to numpy arrays
    val_predictions = np.array(val_predictions)
    val_targets = np.array(val_targets)

    # Calculate confusion matrix
    cm = confusion_matrix(val_targets, val_predictions)

    # Plot confusion matrix
    classes = ['Fake', 'Real']  # Replace with your class labels
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=classes, yticklabels=classes)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

# ...

# Define the model architecture
model_p = models.resnet50(pretrained=True)
model_p.fc = nn.Sequential(
    nn.Linear(in_features=2048, out_features=512, bias=True),
    nn.ReLU(inplace=True),
    nn.Linear(in_features=512, out_features=1, bias=True)
)

for param in model_p.parameters():
    param.requires_grad = True

model_p.to(device)

criterion_p = torch.nn.BCELoss()
optimizer_p = torch.optim.Adam(model_p.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=0.9)

# Call the training_loop function
training_loop(model_p, train_loader, val_loader, criterion_p, optimizer_p, scheduler_p, epochs=num_epochs)

# DOĞRULUK TABLOSUNUN ÇİZDİRİLMESİ

def training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):
    # Training loop code here...

    # After training, calculate predictions on validation set
    model.eval()
    val_predictions = []
    val_targets = []

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            predictions = torch.round(torch.sigmoid(outputs))

            val_predictions.extend(predictions.cpu().numpy())
            val_targets.extend(labels.cpu().numpy())

    # Convert lists to numpy arrays
    val_predictions = np.array(val_predictions)
    val_targets = np.array(val_targets)

    # Calculate classification report
    report = classification_report(val_targets, val_predictions, target_names=['Fake', 'Real'])

    # Display the truth table
    print(report)

# ...

# Define the model architecture
model_p = models.resnet50(pretrained=True)
model_p.fc = nn.Sequential(
    nn.Linear(in_features=2048, out_features=512, bias=True),
    nn.ReLU(inplace=True),
    nn.Linear(in_features=512, out_features=1, bias=True)
)

for param in model_p.parameters():
    param.requires_grad = True

model_p.to(device)

criterion_p = torch.nn.BCELoss()
optimizer_p = torch.optim.Adam(model_p.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=0.9)

# Call the training_loop function
training_loop(model_p, train_loader, val_loader, criterion_p, optimizer_p, scheduler_p, epochs=num_epochs)

import matplotlib.pyplot as plt

def training_loop(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    for epoch in range(epochs):
        # Training loop code here...

        # After each epoch, calculate losses and accuracies
        train_loss = np.mean(train_losses)
        val_loss = np.mean(val_losses)
        train_accuracy = np.mean(train_accuracies)
        val_accuracy = np.mean(val_accuracies)

        # Print the current epoch's statistics
        print(f"Epoch: {epoch+1}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}")
        print("-----------------------------------------")

        # Store the losses and accuracies for visualization
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)

        # ...

    # Plot the accuracy and loss curves
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs+1), val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, epochs+1), val_accuracies, label='Val Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()

# ...

# Define the model architecture
model_p = models.resnet50(pretrained=True)
model_p.fc = nn.Sequential(
    nn.Linear(in_features=2048, out_features=512, bias=True),
    nn.ReLU(inplace=True),
    nn.Linear(in_features=512, out_features=1, bias=True)
)

for param in model_p.parameters():
    param.requires_grad = True

model_p.to(device)

criterion_p = torch.nn.BCELoss()
optimizer_p = torch.optim.Adam(model_p.parameters(), lr=0.00001, weight_decay=1e-5)
scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=0.9)

# Call the training_loop function
training_loop(model_p, train_loader, val_loader, criterion_p, optimizer_p, scheduler_p, epochs=num_epochs)

!pip install torchcam

from torchcam.methods import GradCAMpp
from torchcam.utils import overlay_mask
from torchvision.io.image import read_image
from torchvision.transforms.functional import normalize, resize, to_pil_image

img_path = '/content/drive/MyDrive/data_source/data/real-and-fake-face-detec/real-vs-fake/real-vs-fake/train/fake/001DDU0NI4.jpg'
img = read_image(img_path)
fig = plt.figure(figsize=(8, 8))
plt.imshow(to_pil_image(img))

def saliency_map(image_path, model, shape, model_name):
    '''Drawing saliency heatmap for one loaded image'''
    model.to(device)
    model.eval()


    image = read_image(img_path)

    image = resize(image, (224, 224))
    image = image.clone().detach()
    image = image.unsqueeze(0)
    image = image.to(device)
    image = image.float()
    image.requires_grad_()

    output = model(image)
    output_idx = output.argmax()
    output_max = output[0, output_idx]
    output_max.backward()

    saliency, _ = torch.max(image.grad.data.abs(), dim=1)
    saliency = saliency.reshape(shape)

    fig = plt.figure(figsize=(8, 8))
    plt.imshow(saliency.cpu(), origin='upper', cmap='inferno')
    plt.title(model_name)

def grad_mask_vizualiser(img_path, model, model_name):
    '''GradCAM CNN açıklama yönteminin, sinir ağı için en önemli özelliklerin görüntünün neresinde olduğunu gösteren görselleştirme çizimi'''
    model.eval()
    cam_extractor = GradCAMpp(model, 'layer4')
    img = read_image(img_path)
    input_tensor = normalize(resize(img, (224, 224)) / 255.0, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    input_tensor = input_tensor.to(device)
    out = model(input_tensor.unsqueeze(0).cuda())
    cams = cam_extractor(out.squeeze(0).argmax().item(), out)
    for name, cam in zip(cam_extractor.target_names, cams):
        result = overlay_mask(to_pil_image(img), to_pil_image(cam.squeeze(0), mode='F'), alpha=0.5)
        plt.imshow(result)
        plt.axis('off')
        plt.title(model_name)

grad_mask_vizualiser(img_path, model_np, 'ResNet-50 not pretrained')

saliency_map(img_path, model_np, (224, 224), 'ResNet-50 not pretrained')

grad_mask_vizualiser(img_path, model_p, 'ResNet-50 pretrained')

saliency_map(img_path, model_p, (224, 224), 'ResNet-50 pretrained')

"""denemeeee"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
import numpy as np
from tensorflow.keras import layers
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.callbacks import Callback, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import metrics
import tensorflow as tf
from tensorflow.keras.callbacks import Callback , ReduceLROnPlateau , ModelCheckpoint, CSVLogger
from sklearn.metrics import cohen_kappa_score, accuracy_score
from tensorflow.keras.losses import categorical_crossentropy as logloss
from tensorflow.keras.metrics import categorical_accuracy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import os
import cv2
from PIL import Image
import scipy
import tensorflow as tf
from sklearn import metrics
from sklearn.model_selection import train_test_split
from tqdm import tqdm

import json



base_path = '/content/drive/MyDrive/data_source/data/real-and-fake-face-detec/real-vs-fake/real-vs-fake/'

def plot_img(base_path, set_):
    dir_ = os.path.join(base_path, 'train', set_)
    k = 0
    fig, ax = plt.subplots(3,3, figsize=(10,10))
    fig.suptitle(set_ + 'Faces')
    for j in range(3):
        for i in range(3):
            img = load_img(os.path.join(dir_, os.listdir(os.path.join(dir_))[k]))
            ax[j,i].imshow(img)
            ax[j,i].set_title("")
            ax[j,i].axis('off')
            k +=1
  #  fig.tight_layout()
    plt.suptitle(set_ + ' Faces')
    return plt

plot_img(base_path, 'real').show()

plot_img(base_path, 'fake').show()

ig = ImageDataGenerator(rescale=1./255.)
train_flow = ig.flow_from_directory(
    base_path + 'train/',
    target_size=(128, 128),
    batch_size=64,
    class_mode='categorical'
)

ig1 = ImageDataGenerator(rescale=1./255.)

valid_flow = ig1.flow_from_directory(
    base_path + 'valid/',
    target_size=(128, 128),
    batch_size=64,
    class_mode='categorical'
)

test_flow = ig.flow_from_directory(
    base_path + 'test/',
    target_size=(128, 128),
    batch_size=1,
    shuffle = False,
    class_mode='categorical'
)

train_flow.class_indices

def build_model():
    densenet = ResNet50(
                        weights='imagenet',
                        include_top=False,
                        input_shape=(128,128,3)
                        )
    model = Sequential([densenet,
                        layers.GlobalAveragePooling2D(),
                        layers.Dense(512,activation='relu'),
                        layers.BatchNormalization(),
                        layers.Dense(2, activation='softmax')
                        ])
    model.compile(optimizer=Adam(lr=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy']
                 )
    return model

model=build_model()
model.summary()

train_steps = 500//6
valid_steps = 100//6

history = model.fit(train_flow,
    epochs = 3,
    steps_per_epoch =train_steps,
    validation_data =valid_flow,
    validation_steps = valid_steps,
    callbacks=callbacks
)

def build_model():
    densenet = ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 128, 3)
    )
    model = Sequential([
        densenet,
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(2, activation='softmax')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

model = build_model()
model.summary()

checkpoint = ModelCheckpoint(
    filepath='model.h5',
    save_best_only=True,
    verbose=1,
    mode='min',
    monitor='val_loss'
)
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    verbose=1,
    min_delta=0.0001
)
csv_logger = CSVLogger('training.log')

callbacks = [checkpoint, reduce_lr, csv_logger]

# Calculate steps per epoch
train_steps = len(train_flow)
valid_steps = len(valid_flow)

# Training loop
history = model.fit(
    train_flow,
    epochs=2,
    steps_per_epoch=train_steps,
    validation_data=valid_flow,
    validation_steps=valid_steps,
    callbacks=callbacks
)

# Print epoch, accuracy, and loss values
for epoch in range(len(history.history['accuracy'])):
    acc = history.history['accuracy'][epoch]
    loss = history.history['loss'][epoch]
    print(f"Epoch {epoch+1}: Accuracy = {acc}, Loss = {loss}")

plt.figure(figsize=(14,5))
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['train', 'val'])

plt.subplot(1,2,1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['train', 'val'])
plt.show()